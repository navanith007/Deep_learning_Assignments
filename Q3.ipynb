{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on the rotten tomatoes data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PhraseId  SentenceId                                             Phrase  \\\n",
      "0         1           1  A series of escapades demonstrating the adage ...   \n",
      "1         2           1  A series of escapades demonstrating the adage ...   \n",
      "2         3           1                                           A series   \n",
      "3         4           1                                                  A   \n",
      "4         5           1                                             series   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n",
      "['PhraseId', 'SentenceId', 'Phrase', 'Sentiment']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"mrdata.tsv\", delimiter = '\\t')\n",
    "print(data.head())\n",
    "print(list(data.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PhraseId  SentenceId                                             Phrase  \\\n",
      "0         1           1  a series of escapades demonstrating the adage ...   \n",
      "1         2           1  a series of escapades demonstrating the adage ...   \n",
      "2         3           1                                           a series   \n",
      "3         4           1                                                  a   \n",
      "4         5           1                                             series   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n",
      "(156060, 4)\n"
     ]
    }
   ],
   "source": [
    "data2 = data.apply(lambda x: x.astype(str).str.lower())\n",
    "data2['PhraseId'] = pd.to_numeric(data2['PhraseId'])\n",
    "data2['SentenceId'] = pd.to_numeric(data2['SentenceId'])\n",
    "data2['Sentiment'] = pd.to_numeric(data2['Sentiment'])\n",
    "print(data2.head())\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PhraseId  SentenceId                                             Phrase  \\\n",
      "0         1           1  a series of escapades demonstrating the adage ...   \n",
      "1         2           1  a series of escapades demonstrating the adage ...   \n",
      "2         3           1                                           a series   \n",
      "3         4           1                                                  a   \n",
      "4         5           1                                             series   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n",
      "<class 'pandas.core.series.Series'>\n",
      "a series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n"
     ]
    }
   ],
   "source": [
    "#data2['Phrase'] = data2['Phrase'].str.replace('\\W', '')\n",
    "print(data2.head())\n",
    "print(type(data2['Phrase']))\n",
    "print(data2.iloc[0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "#print(len(data2.index))\n",
    "for i in range(len(data2.index)):\n",
    "    texts.append(data2.iloc[i,2])\n",
    "    labels.append(data2.iloc[i,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15288 unique tokens.\n",
      "Shape of data : (156060, 100)\n",
      "Shape of label: (156060,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100\n",
    "max_words = 15288\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data :', data.shape)\n",
    "print('Shape of label:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Parsing the GloVe word-embeddings file\n",
    "\n",
    "For embedding the data set glove dataset with 600 dimensional is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'C:/Users/Navaneeth/Documents/Deep Learning/Assignments/Question3/IMDB'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Preparing the GloVe word-embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15288, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "Accuracy = 49.32456\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   2 323]\n",
      "[2, 323]\n",
      "a series\n",
      "[[0.96875309 1.50331128]\n",
      " [0.75704193 1.52072373]]\n"
     ]
    }
   ],
   "source": [
    "print(data[2])\n",
    "print(sequences[2])\n",
    "print(data2.iloc[2, 2])\n",
    "x = np.random.random((2, 4))\n",
    "y = np.random.random((4, 2))\n",
    "print(np.dot(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Modeling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):  \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):  \n",
    "    return sigmoid(x) *(1-sigmoid (x))\n",
    "\n",
    "def softmax(A):  \n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = np.zeros((data.shape[0], 5))\n",
    "\n",
    "for i in range(data.shape[0]):  \n",
    "    one_hot_labels[i, labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = data.shape[0]\n",
    "tra_len = int(100)\n",
    "train = data[:tra_len]\n",
    "test = data[int(0.95*tra_len):]\n",
    "train_label = labels[:tra_len]\n",
    "test_label = labels[int(0.95*tra_len):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epho = 1\n",
    "word_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "      #--------------------------------initiallising params of LSTM layer--------------------#\n",
    "\n",
    "#----------initiallising params for input_activation---------------\n",
    "W_a = np.random.random((output_dim, word_dim))\n",
    "U_a = np.random.random((output_dim,output_dim))\n",
    "b_a = np.random.random((output_dim,))\n",
    "\n",
    "#----------intilallising params for input_gate---------------------\n",
    "W_i = np.random.random((output_dim, word_dim))\n",
    "U_i = np.random.random((output_dim,output_dim))\n",
    "b_i = np.random.random((output_dim,))\n",
    "\n",
    "#----------intiallising params for forget_gate---------------------\n",
    "W_f = np.random.random((output_dim, word_dim))\n",
    "U_f = np.random.random((output_dim,output_dim))\n",
    "b_f = np.random.random((output_dim,))\n",
    "\n",
    "#----------intiallising params for output_gate---------------------\n",
    "W_o = np.random.random((output_dim, word_dim))\n",
    "U_o = np.random.random((output_dim,output_dim))\n",
    "b_o = np.random.random((output_dim,))\n",
    "\n",
    "      #---------------------------------intiallising params of classification layer------------#\n",
    "\n",
    "#-------initiallising params for input to hidden--------\n",
    "hidden_units = 5\n",
    "W_hi = np.random.random((hidden_units, output_dim)) #'i' indicates input,  'h' hidden units\n",
    "b_h = np.random.random((hidden_units,))\n",
    "\n",
    "#-------initiallising params for hidden to classification------\n",
    "classification_units = 5\n",
    "W_ch = np.random.random((classification_units, hidden_units)) # 'c' indicates output, 'h' indicates hidden units\n",
    "b_c = np.random.random((classification_units,))\n",
    "\n",
    "learning_rate = 0.1\n",
    "Z_h = []\n",
    "a_h = []\n",
    "Z_c = []\n",
    "a_c = []\n",
    "feature = []\n",
    "for i in range(epho):\n",
    "    Xt = []\n",
    "    At = []\n",
    "    It = []\n",
    "    Ft = []\n",
    "    Ot = []\n",
    "    State =[]\n",
    "    Output =[]\n",
    "    #feed_forward_propagation\n",
    "    for j in range(train.shape[0]):\n",
    "        input = data[j] # The sequence of tokenized input\n",
    "        #------intiallising initial internal state and output-----------\n",
    "        state_t = np.zeros((output_dim,))\n",
    "        output_t = np.zeros((output_dim,))\n",
    "        Xt.append(j)\n",
    "        At.append(j)\n",
    "        It.append(j)\n",
    "        Ft.append(j)\n",
    "        Ot.append(j)\n",
    "        State.append(j)\n",
    "        Output.append(j)\n",
    "        \n",
    "        Xt[j] = []\n",
    "        At[j] = []\n",
    "        It[j] = []\n",
    "        Ft[j] = []\n",
    "        Ot[j] = []\n",
    "        State[j] = []\n",
    "        Output[j] = []\n",
    "        \n",
    "        for token_index in input:\n",
    "            x_t = embedding_matrix[token_index]\n",
    "            \n",
    "            a_t = np.tanh(np.dot(W_a, x_t) + np.dot(U_a, output_t) + b_a) # Input activation\n",
    "            i_t = sigmoid(np.dot(W_i, x_t) + np.dot(U_i, output_t) + b_i) # Input gate\n",
    "            f_t = sigmoid(np.dot(W_f, x_t) + np.dot(U_f, output_t) + b_f) # forget gate\n",
    "            o_t = sigmoid(np.dot(W_o, x_t) + np.dot(U_o, output_t) + b_o) # output_gate\n",
    "            \n",
    "            Xt[j].append(x_t)\n",
    "            At[j].append(a_t)\n",
    "            It[j].append(i_t)\n",
    "            Ft[j].append(f_t)\n",
    "            Ot[j].append(o_t)\n",
    "            State[j].append(state_t)\n",
    "            Output[j].append(output_t)\n",
    "\n",
    "            state_t = np.multiply(a_t, i_t) + np.multiply(f_t, state_t) # internal state\n",
    "            output_t = np.multiply(np.tanh(state_t), o_t) # internal output\n",
    "            \n",
    "            \n",
    "        feature.append(output_t)\n",
    "        Z_h.append(np.dot(W_hi, output_t) + b_h)\n",
    "        a_h.append(sigmoid(np.dot(W_hi, output_t) + b_h))\n",
    "        \n",
    "        Z_c.append(np.dot(W_ch, sigmoid(np.dot(W_hi, output_t) + b_h)) + b_c)\n",
    "        a_c.append(softmax(np.dot(W_ch, sigmoid(np.dot(W_hi, output_t) + b_h)) + b_c))\n",
    "        \n",
    "    #----------------------- backward_propagation----------------------------\n",
    "    dcost_dwc = a_c - one_hot_labels[:tra_len] # n*5\n",
    "    dzc_dwc = a_h # n*h\n",
    "    dcost_wc = np.dot(dcost_dwc.T, dzc_dwc)\n",
    "    dcost_bc = dcost_dwc #n*5\n",
    "    \n",
    "    dzc_dah = W_ch # 5*h\n",
    "    dcost_dah = np.dot(dcost_dwc, dzc_dah) # n*h\n",
    "    \n",
    "    dah_dzh = np.zeros((train.shape[0], hidden_units))\n",
    "    for j in range(train.shape[0]):\n",
    "        dah_dzh[j] = sigmoid_der(Z_h[j]) # n*h\n",
    "    \n",
    "    dzh_dwh = feature # n*10\n",
    "    dcost_wh = np.dot( (dah_dzh * dcost_dah).T , dzh_dwh,) #h*10\n",
    "    dcost_bh = dcost_dah * dah_dzh #n*h\n",
    "    \n",
    "    dzh_df = W_hi # h*10\n",
    "    dcost_df = np.dot(dah_dzh * dcost_dah, dzh_df) #n*10\n",
    "    \n",
    "    #updating classification weights\n",
    "    W_hi -= learning_rate * dcost_wh\n",
    "    b_h -= learning_rate * dcost_bh.sum(axis=0)\n",
    "\n",
    "    W_ch -= learning_rate * dcost_wc\n",
    "    b_c -= learning_rate * dcost_bc.sum(axis=0)\n",
    "    \n",
    "    #-----------back_propagation on LSTM layer-----------------\n",
    "    delta_Wa = np.zeros((output_dim, word_dim))\n",
    "    delta_Wi = np.zeros((output_dim, word_dim))\n",
    "    delta_Wf = np.zeros((output_dim, word_dim))\n",
    "    delta_Wo = np.zeros((output_dim, word_dim))\n",
    "    \n",
    "    delta_Ua = np.zeros((output_dim, output_dim))\n",
    "    delta_Ui = np.zeros((output_dim, output_dim))\n",
    "    delta_Uf = np.zeros((output_dim, output_dim))\n",
    "    delta_Uo = np.zeros((output_dim, output_dim))\n",
    "    \n",
    "    delta_ba = np.zeros((output_dim, ))\n",
    "    delta_bi = np.zeros((output_dim, ))\n",
    "    delta_bf = np.zeros((output_dim, ))\n",
    "    delta_bo = np.zeros((output_dim, ))\n",
    "    \n",
    "    Delta_t = learning_rate * dcost_df\n",
    "    \n",
    "    for j in range(train.shape[0]):\n",
    "        \n",
    "        Delta_out = np.zeros((output_dim, ))\n",
    "        delta_state = np.zeros((output_dim, ))\n",
    "\n",
    "        for t in reversed(range(data.shape[1])):\n",
    "            delta_out = Delta_t[j] + Delta_out\n",
    "            delta_state = delta_out * Ot[j][t] * 1 - (np.tanh(State[j][t])**2)  + delta_state * Ft[j][t]\n",
    "            delta_at = delta_state * It[j][t] * (1- (At[j][t]**2))\n",
    "            #print(delta_at.shape)\n",
    "            delta_it = delta_state * At[j][t] * It[j][t] * (1- (It[j][t]**2))\n",
    "            #print(delta_it.shape)\n",
    "            delta_ft = delta_state * State[j][t-1] * Ft[j][t] * (1- Ft[j][t])\n",
    "            #print(delta_ft.shape)\n",
    "            delta_ot = delta_out * np.tanh(State[j][t]) * Ot[j][t] * (1- Ot[j][t])\n",
    "            #print(delta_ot.shape)\n",
    "            delta_gate = []\n",
    "            delta_gate.append(delta_at)\n",
    "            delta_gate.append(delta_it)\n",
    "            delta_gate.append(delta_ft)\n",
    "            delta_gate.append(delta_ot)\n",
    "            #delta_gate = np.asarray(delta_gate)\n",
    "            #print(U_a.shape)\n",
    "            Delta_out += np.dot(U_a, delta_at)\n",
    "            Delta_out += np.dot(U_i,  delta_it)\n",
    "            Delta_out += np.dot(U_f, delta_ft)\n",
    "            Delta_out += np.dot(U_o, delta_ot)\n",
    "\n",
    "            delta_Wa += np.outer(delta_at, Xt[j][t])\n",
    "            delta_Wi += np.outer(delta_it, Xt[j][t])\n",
    "            delta_Wf += np.outer(delta_ft, Xt[j][t])\n",
    "            delta_Wo += np.outer(delta_ot, Xt[j][t])\n",
    "\n",
    "            delta_Ua += np.outer(delta_at, Output[j][t])\n",
    "            delta_Ui += np.outer(delta_it, Output[j][t])\n",
    "            delta_Uf += np.outer(delta_ft, Output[j][t])\n",
    "            delta_Uo += np.outer(delta_ot, Output[j][t])\n",
    "\n",
    "            delta_ba += delta_at\n",
    "            delta_bi += delta_it\n",
    "            delta_bf += delta_ft\n",
    "            delta_bo += delta_ot\n",
    "            \n",
    "    W_a -= learning_rate*delta_Wa\n",
    "    W_i -= learning_rate*delta_Wi\n",
    "    W_f -= learning_rate*delta_Wf\n",
    "    W_o -= learning_rate*delta_Wo\n",
    "    \n",
    "    U_a -= learning_rate*delta_Ua\n",
    "    U_i -= learning_rate*delta_Ui\n",
    "    U_f -= learning_rate*delta_Uf\n",
    "    U_o -= learning_rate*delta_Uo\n",
    "    \n",
    "    b_a -= learning_rate*delta_ba\n",
    "    b_i -= learning_rate*delta_bi\n",
    "    b_f -= learning_rate*delta_bf\n",
    "    b_o -= learning_rate*delta_bo\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting the labels of test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = []\n",
    "for j in range(test.shape[0]):\n",
    "    input = test[j] # The sequence of tokenized input\n",
    "    #------initiallising initial internal state and output-----------\n",
    "    state_t = np.zeros((output_dim,))\n",
    "    output_t = np.zeros((output_dim,))\n",
    "    Xt.append(j)\n",
    "    At.append(j)\n",
    "    It.append(j)\n",
    "    Ft.append(j)\n",
    "    Ot.append(j)\n",
    "    State.append(j)\n",
    "    Output.append(j)\n",
    "\n",
    "    Xt[j] = []\n",
    "    At[j] = []\n",
    "    It[j] = []\n",
    "    Ft[j] = []\n",
    "    Ot[j] = []\n",
    "    State[j] = []\n",
    "    Output[j] = []\n",
    "\n",
    "    for token_index in input:\n",
    "        x_t = embedding_matrix[token_index]\n",
    "\n",
    "        a_t = np.tanh(np.dot(W_a, x_t) + np.dot(U_a, output_t) + b_a) # Input activation\n",
    "        i_t = sigmoid(np.dot(W_i, x_t) + np.dot(U_i, output_t) + b_i) # Input gate\n",
    "        f_t = sigmoid(np.dot(W_f, x_t) + np.dot(U_f, output_t) + b_f) # forget gate\n",
    "        o_t = sigmoid(np.dot(W_o, x_t) + np.dot(U_o, output_t) + b_o) # output_gate\n",
    "        \n",
    "        state_t = np.multiply(a_t, i_t) + np.multiply(f_t, state_t) # internal state\n",
    "        output_t = np.multiply(np.tanh(state_t), o_t) # internal output\n",
    "\n",
    "\n",
    "    #feature.append(output_t)\n",
    "    Z_h = np.dot(W_hi, output_t) + b_h\n",
    "    a_h = sigmoid(np.dot(W_hi, output_t) + b_h)\n",
    "\n",
    "    Z_c = np.dot(W_ch, sigmoid(np.dot(W_hi, output_t) + b_h)) + b_c\n",
    "    a_c = softmax(np.dot(W_ch, sigmoid(np.dot(W_hi, output_t) + b_h)) + b_c)\n",
    "    #print(a_c.shape)\n",
    "    a_c = np.asarray(a_c)\n",
    "    label_pred.append(np.argmax(a_c))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.977462892315586\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(test_label == label_pred) / test.shape[0]\n",
    "print(accuracy*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
